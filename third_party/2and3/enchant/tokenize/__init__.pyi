# from enchant.errors import *
from typing import Any, Callable, Iterable, Iterator, Optional, Tuple, Type, Union
from typing_extensions import Protocol

Token = Tuple[str, int]

# Error = TokenizerNotFoundError

class tokenize(Protocol):
    def __init__(self, text: str) -> None: ...
    def __next__(self) -> Token: ...
    def next(self) -> Token: ...
    def __iter__(self) -> Iterator[Token]: ...
    def set_offset(self, offset: int, replaced: bool = ...) -> None: ...
    offset: int = ...

class empty_tokenize(tokenize):
    def __init__(self) -> None: ...

class unit_tokenize(tokenize):
    def __init__(self, text: str) -> None: ...

class basic_tokenize(tokenize):
    strip_from_start: str = ...
    strip_from_end: str = ...

class Chunker(tokenize): ...

_Filter = Union[Type[tokenize], Filter]

class Filter(tokenize):
    def __init__(self, tokenizer: _Filter) -> None: ...
    def __call__(self, *args: Any, **kwds: Any) -> tokenize: ...
    def _skip(self, word: str) -> bool: ...
    def _split(self, word: str) -> tokenize: ...
    class _TokenFilter(tokenize):
        def __init__(self, tokenizer: _Filter, skip: Callable[[str], bool], split: Callable[[str], tokenize]) -> None: ...

def wrap_tokenizer(tk1: _Filter, tk2: _Filter) -> Filter: ...

class URLFilter(Filter): ...
class WikiWordFilter(Filter): ...
class EmailFilter(Filter): ...
class MentionFilter(Filter): ...
class HashtagFilter(Filter): ...

class HTMLChunker(Chunker):
    def next(self) -> Token: ...

def get_tokenizer(tag: str = ..., chunkers: Iterable[Chunker] = ..., filters: Iterable[Filter] = ...) -> tokenize: ...
